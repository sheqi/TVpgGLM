{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder Tutorial in TensorFlow \n",
    "## David Zoltowski"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorial is organized in the following manner:\n",
    "1. Set-up: importing packages, data, defining useful functions, and parameter settings\n",
    "2. Building the computational graph: defining the series of operations we perform on an input. This is where we build the encoder and decoder networks and implement the reparametrization trick. Additionally, we define the loss function used to optimize the parameters of the graph.\n",
    "3. Training the model: we train the model using stochastic gradient descent on mini-batches of data, optimizing the loss function. \n",
    "4. Visualization: visualize the ability of the trained models to reconstruct test images and sampling new images from the prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set-up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the necessary packages, numpy and tensorflow, and the data on which we will train the model, MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "\n",
    "# import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When defining the model we will define many sets of weight and bias parameters (variables). I have copied this function from the TensorFlow website to define normal distributed weight variables, truncated at 2 standard deviations, and constant bias variables. From the TensorFlow website: \"Variables allow us to add trainable parameters to a graph. They are constructed with a type and initial value\" e.g. tf.Variable([0.0],dtype=tf.float32)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define functions to create weight and bias variables, from TensorFlow.org\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final part of the set-up is to define the size of different parts of our network. We specifiy the input size to be the length of a image stacked into a vector, the number of hidden units in each of our hidden layers, and the dimensionality of the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latent_dim = 20; # size of latent space\n",
    "input_size = 784; # size of input image vector\n",
    "hidden_size = 500; # size of hidden layers in neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Building the computational graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the placeholder construct in TensorFlow to parametrize the graph to accept inputs (TensorFlow.org). The placeholders fixes a node in the computational graph with no value, but a value which we will specify later. \n",
    "\n",
    "We define the placeholder with the type of input, here a floating point tf.float32, and size of input. Our input to placeholders will be a mini-batch of images. We specify the size following the TensorFlow webiste: \"We want to be able to input any number of MNIST images, each flattened into a 784-dimensional vector. We represent this as a 2-D tensor of floating-point numbers, with a shape [None, 784]. (Here None means that a dimension can be of any length.)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 784])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next stage of the VAE is to build the decoder network, also called the inference network or approximate posterior. This network will map the image vector to two vectors of size \"latent_dim\": a mean vector and a log variance vector. \n",
    "\n",
    "I created a network with two hidden layers. I defined two sets of hidden layer weights, W1 and W2, and two sets of hidden layer biases, b1 and b2. The weights are matrices with dimensionality \"input layer x current layer\" and the biases are vectors with dimensionality \"current layer\". For each layer, I used the \"tf.matmul\" function to multiply the preceding input and the weight matrices, and then added the biases. Finally, I passed the sum W \\* x + b through a sigmoidal nonlinearity using \"tf.nn.sigmoid\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# encoder network\n",
    "W1 = weight_variable([input_size,hidden_size])\n",
    "b1 = bias_variable([hidden_size])\n",
    "h1 = tf.nn.sigmoid(tf.matmul(x, W1) + b1)\n",
    "W2 = weight_variable([hidden_size,hidden_size])\n",
    "b2 = bias_variable([hidden_size])\n",
    "h2 = tf.nn.sigmoid(tf.matmul(h1,W2) + b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to read out the second hidden layer to the mean vector and log variance vector. I introduce two more sets of weights and biases corresponding to each output vector. The output vectors are not passed through a nonlinearity. I chose to output the log variance so that I did not have to worry about positivity constraints. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get mean\n",
    "W_hidden_mean = weight_variable([hidden_size, latent_dim])\n",
    "b_hidden_mean = bias_variable([latent_dim])\n",
    "hidden_mean = tf.matmul(h2, W_hidden_mean) + b_hidden_mean\n",
    "\n",
    "# get sigma - log variances\n",
    "W_hidden_sigma = weight_variable([hidden_size, latent_dim])\n",
    "b_hidden_sigma = bias_variable([latent_dim])\n",
    "hidden_log_sigma_sqr = tf.matmul(h2, W_hidden_sigma) + b_hidden_sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preceding layers will output a mean vector and log variance vector for every input image in the batch, corresponding to the mean and variance parameters of the approximate posterior for each image. We will train the parameters of the variatonal autoencoder - both the parameters of the approximate posterior q(z|x) and generative model p(x|z) which we have yet to define - using stochastic estimates of the evidence lower bound (ELBO): E_q(z|x)[log p(x|z)] - D_KL[q(z|x)||p(z)], where p(z) is N(0,I). \n",
    "\n",
    "We will estimate the ELBO expectation using one sample from q(z|x) for each data point. This is where we use the reparametrization trick obtain lower variance gradients. Instead, we will draw noise eps from a zero mean, identity covariance Gaussian and map it to a sample from each q(z|x) using the mean and log variance vectors: z = mu + eps.\\* variance.\n",
    "\n",
    "We draw a noise vector eps and map it to a sample from the posterior q(z|x) for each input image x in the mini-batch with the following lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sample noise with same shape as log variance vectors\n",
    "eps = tf.random_normal(tf.shape(hidden_log_sigma_sqr), 0, 1, dtype=tf.float32)\n",
    "\n",
    "# get a sample from the approximate posterior for each input\n",
    "# add mean and the std (sqrt of the exponentiated log variances) pointwise times noise\n",
    "hidden_sample = hidden_mean + tf.multiply(tf.sqrt(tf.exp(hidden_log_sigma_sqr)),eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a sample z from the approximate posterior q(z|x), we pass z through a decoder network to reconstruct the image. I defined the decoder network similarly to the same way as the encoder network, with two hidden layers. However, this time the input to the network is of dimensionality \"latent_dim\" and the output of dimension 784 x 1, the image size, is passed through a sigmoidal function to be in [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# decoder network - map the hidden sample to an output of size = image size\n",
    "W3 = weight_variable([latent_dim,hidden_size])\n",
    "b3 = bias_variable([hidden_size])\n",
    "h3 = tf.nn.sigmoid(tf.matmul(hidden_sample, W3) + b3)\n",
    "W4 = weight_variable([hidden_size,hidden_size])\n",
    "b4 = bias_variable([hidden_size])\n",
    "h4 = tf.nn.sigmoid(tf.matmul(h3,W4) + b4)\n",
    "\n",
    "# output x_hat, the reconstruction mean\n",
    "W_out = weight_variable([hidden_size,input_size])\n",
    "b_out = bias_variable([input_size])\n",
    "x_hat = tf.nn.sigmoid(tf.matmul(h4, W_out) + b_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last part of the computational graph that we need to specify is the training objective. The objective consists of two parts: the reconstruction loss - corresponding go log p(x|z) - and the KL divergence D_KL( q(z|x) || p(z) ).\n",
    "\n",
    "I will set the reconstruction loss to be the sum of squared errors between each pixel of the input image x and output of the network x_hat. This corresponds to log p(x|z) up to an additive constant when p(x|z) is N(mu(z),I)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reconstruction loss is squared error between reconstruction and image (MLE in N(mu(x),sigma^2 I))\n",
    "reconstruction_loss = tf.reduce_sum(tf.square(x-x_hat)/0.5,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A loss that I have observed being used is the cross entropy of the output reconstruction and the true image value, which is mathematically equivalent to the Bernoulli log likelihood of each pixel of image x given each parameter x_hat. However, we are confused as to why this loss is being use, as the pixel values are not binary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# another loss people use is the cross entropy of the output reconstruction and true image value\n",
    "# reconstruction loss is Bernoulli log likelihood of each pixel of image x given x hat (output of decoder)\n",
    "#reconstruction_loss = -tf.reduce_sum(x * tf.log(1e-10 + x_hat) + (1-x) * tf.log(1e-10 + 1 - x_hat),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KL divergence between two normal distributions q(z|x) and p(z) has an exact form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# KL divegence between the approximate posterior and prior\n",
    "kl_divergence = -0.5 * tf.reduce_sum(1 + hidden_log_sigma_sqr - tf.square(hidden_mean) - tf.exp(hidden_log_sigma_sqr), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training objective \"loss\" is the mean of the reconstruction loss and KL divergence loss across the images in the mini-batch. We will train the variables in our model to minimize the loss, using the tf.train API. I have defined a training step below which will use the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# avg_loss is the mean across images x in the batch\n",
    "loss = tf.reduce_mean(reconstruction_loss + kl_divergence);\n",
    "\n",
    "# train step\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Training the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training code was modified from https://jmetzen.github.io/2015-11-27/vae.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss= 101.710126051\n",
      "Epoch: 0002 loss= 74.687621460\n",
      "Epoch: 0003 loss= 62.680545023\n",
      "Epoch: 0004 loss= 58.401112726\n",
      "Epoch: 0005 loss= 55.899328676\n",
      "Epoch: 0006 loss= 54.053136749\n",
      "Epoch: 0007 loss= 52.529357328\n",
      "Epoch: 0008 loss= 51.337708685\n",
      "Epoch: 0009 loss= 50.435387976\n",
      "Epoch: 0010 loss= 49.560607688\n",
      "Epoch: 0011 loss= 48.787097952\n",
      "Epoch: 0012 loss= 48.163268800\n",
      "Epoch: 0013 loss= 47.568813594\n",
      "Epoch: 0014 loss= 46.933215880\n",
      "Epoch: 0015 loss= 46.377606236\n",
      "Epoch: 0016 loss= 45.978230202\n",
      "Epoch: 0017 loss= 45.603746449\n",
      "Epoch: 0018 loss= 45.278219813\n",
      "Epoch: 0019 loss= 45.043410381\n",
      "Epoch: 0020 loss= 44.720613806\n",
      "Epoch: 0021 loss= 44.492978654\n",
      "Epoch: 0022 loss= 44.294472053\n",
      "Epoch: 0023 loss= 44.048974665\n",
      "Epoch: 0024 loss= 43.889628795\n",
      "Epoch: 0025 loss= 43.640362140\n",
      "Epoch: 0026 loss= 43.510595273\n",
      "Epoch: 0027 loss= 43.327448203\n",
      "Epoch: 0028 loss= 43.154704056\n",
      "Epoch: 0029 loss= 43.003979319\n",
      "Epoch: 0030 loss= 42.821728224\n",
      "Epoch: 0031 loss= 42.703358057\n",
      "Epoch: 0032 loss= 42.536656369\n",
      "Epoch: 0033 loss= 42.396530470\n",
      "Epoch: 0034 loss= 42.276359086\n",
      "Epoch: 0035 loss= 42.130464221\n",
      "Epoch: 0036 loss= 42.015516066\n",
      "Epoch: 0037 loss= 41.914743937\n",
      "Epoch: 0038 loss= 41.758315991\n",
      "Epoch: 0039 loss= 41.607271354\n",
      "Epoch: 0040 loss= 41.509473336\n",
      "Epoch: 0041 loss= 41.382994655\n",
      "Epoch: 0042 loss= 41.248079432\n",
      "Epoch: 0043 loss= 41.193113424\n",
      "Epoch: 0044 loss= 41.081081238\n",
      "Epoch: 0045 loss= 40.977247148\n",
      "Epoch: 0046 loss= 40.890645058\n",
      "Epoch: 0047 loss= 40.766621212\n",
      "Epoch: 0048 loss= 40.693657442\n",
      "Epoch: 0049 loss= 40.613493368\n",
      "Epoch: 0050 loss= 40.534157701\n"
     ]
    }
   ],
   "source": [
    "# saver to save model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# train network\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "n_samples = mnist.train.num_examples\n",
    "# Training cycle\n",
    "training_epochs = 50\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "for epoch in range(training_epochs):\n",
    "    avg_loss = 0.\n",
    "    total_batch = int(n_samples / batch_size)\n",
    "    # Loop over all batches\n",
    "    for i in range(total_batch):\n",
    "        x_batch = mnist.train.next_batch(batch_size)\n",
    "        current_loss = loss.eval(feed_dict={x:x_batch[0]}) #/ n_samples * batch_size\n",
    "        # Compute average loss\n",
    "        avg_loss += current_loss / n_samples * batch_size\n",
    "        # Fit training using batch data\n",
    "        train_step.run(feed_dict={x: x_batch[0]})\n",
    "    # Display logs per epoch step\n",
    "    if epoch % display_step == 0:\n",
    "        print(\"Epoch:\", '%04d' % (epoch+1), \n",
    "            \"loss=\", \"{:.9f}\".format(avg_loss))\n",
    "\n",
    "#saver.save(sess, 'davidz_vae_150_epochs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#saver = tf.train.import_meta_graph('davidz_vae_150_epochs.meta')\n",
    "#saver.restore(sess,tf.train.latest_checkpoint('./'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This piece of code takes a test image from MNIST (x_sample) and evaluates the reconstruction x_hat when x_sample is fed into the VAE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAagAAAEYCAYAAAAJeGK1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11be42780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# reconstruct\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "x_sample = mnist.test.next_batch(1)[0]\n",
    "x_reconstruct = x_hat.eval(feed_dict={x:x_sample})\n",
    "plt.figure\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(x_sample.reshape(28, 28), vmin=0, vmax=1, cmap=\"gray\")\n",
    "plt.title(\"Test input\")\n",
    "plt.colorbar()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(x_reconstruct.reshape(28, 28), vmin=0, vmax=1, cmap=\"gray\")\n",
    "plt.title(\"Reconstruction\")\n",
    "plt.colorbar()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUIAAAEYCAYAAAApuP8NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/fzeIAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b94aa20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generate\n",
    "eps = np.random.normal(0,1,size=latent_dim)\n",
    "eps = np.array(eps).reshape(1,latent_dim)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "x_reconstruct = x_hat.eval(feed_dict={hidden_sample:eps})\n",
    "plt.figure\n",
    "plt.imshow(x_reconstruct.reshape(28, 28), vmin=0, vmax=1, cmap=\"gray\")\n",
    "plt.title(\"Test input\")\n",
    "plt.colorbar()\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to add normalizing flows, need to estimate entropy of log posterior rather than compute KL explicitly, \n",
    "# compute probability of latent variable explicitly rather than in KL, \n",
    "# compute Jacobian determinant terms for penalizing ELBO\n",
    "# ensure flows are parametrized in a good way"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
